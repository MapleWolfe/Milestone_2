{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiMTYDR9PoEDvAOMxiQ2tD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MapleWolfe/Milestone_2/blob/main/Supervised_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised learning notebook"
      ],
      "metadata": {
        "id": "meBNq_puwEX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## installs, imports, pre-sets"
      ],
      "metadata": {
        "id": "Uv1syv3gdvb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "#!python rapidsai-csp-utils/colab/pip-install.py\n",
        "#!pip install google-cloud-storage"
      ],
      "metadata": {
        "id": "qudk0RcgDLyx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "2f960bdb-2462-4a8f-f2e8-4281915a4951"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xgboost==0.90\n",
            "  Downloading xgboost-0.90-py2.py3-none-manylinux1_x86_64.whl (142.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.8/142.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost==0.90) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost==0.90) (1.10.1)\n",
            "Installing collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 1.7.5\n",
            "    Uninstalling xgboost-1.7.5:\n",
            "      Successfully uninstalled xgboost-1.7.5\n",
            "Successfully installed xgboost-0.90\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "xgboost"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_fG6RA56XD_4"
      },
      "outputs": [],
      "source": [
        "#google import options\n",
        "#from google.colab import drive\n",
        "from google.cloud import storage\n",
        "\n",
        "#general usage imports\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import gc\n",
        "import os\n",
        "import multiprocessing\n",
        "import pickle\n",
        "import json\n",
        "import joblib\n",
        "\n",
        "#model operations imports\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#sklearn classifiers\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#GPU imports\n",
        "import cudf\n",
        "from cuml.naive_bayes import GaussianNB\n",
        "from cuml.naive_bayes import ComplementNB\n",
        "from cuml import LogisticRegression\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "\n",
        "import cupy\n",
        "import xgboost as xgb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP set up"
      ],
      "metadata": {
        "id": "TnpY5qVHuE9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/organic-reef-390716-609989a4c6da.json'\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket('fire_train_eval_test_bucket')\n",
        "blob = bucket.blob('test.csv')\n",
        "blob.download_to_filename('test.csv')\n",
        "blob = bucket.blob('eval.csv')\n",
        "blob.download_to_filename('eval.csv')\n",
        "blob = bucket.blob('train.csv')\n",
        "blob.download_to_filename('train.csv')"
      ],
      "metadata": {
        "id": "Wi1hP1Hbs7sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to load and clean csv chunks"
      ],
      "metadata": {
        "id": "B_yF4lBdd2_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remember to add .csv at the end of file name\n",
        "def read_csv_in_chunks(file_name,number_images):\n",
        "\n",
        "  #number of rows per image:\n",
        "  pixels_count = 64*64\n",
        "\n",
        "  #upto 200 images at a time\n",
        "  size = number_images*pixels_count\n",
        "\n",
        "  #file string and location for Google cloud storage\n",
        "  file_string = '/content/' + file_name\n",
        "  return pd.read_csv(file_string, chunksize=size)\n",
        "\n",
        "def read_full_csv(file_name):\n",
        "\n",
        "  #file string and location for Google cloud storage\n",
        "  file_string = '/content/' + file_name\n",
        "\n",
        "\n",
        "  return pd.read_csv(file_string)\n",
        "\n",
        "#this is procedure that cleans the data.\n",
        "# cleaner_1 drops all negative \"firemask\" values and converts all values above 0 to 1\n",
        "def cleaner_1(df_chunk):\n",
        "  col_list = ['NDVI_scaled_smoothened_values', 'NDVI_local_gradient', 'NDVI_local_mean', 'tmmn_scaled_smoothened_values', 'tmmn_local_gradient', 'tmmn_local_mean', 'elevation_scaled_smoothened_values', 'elevation_local_gradient', 'elevation_local_mean', 'fire_at_similar_altitude', 'population_scaled_smoothened_values', 'population_local_gradient', 'population_local_mean', 'vs_scaled_smoothened_values', 'vs_local_gradient', 'vs_local_mean', 'pdsi_scaled_smoothened_values', 'pdsi_local_gradient', 'pdsi_local_mean', 'pr_scaled_smoothened_values', 'pr_local_gradient', 'pr_local_mean', 'tmmx_scaled_smoothened_values', 'tmmx_local_gradient', 'tmmx_local_mean', 'sph_scaled_smoothened_values', 'sph_local_gradient', 'sph_local_mean', 'th_scaled_smoothened_values', 'th_local_gradient', 'th_local_mean', 'distance_from_fire', 'erc_scaled_smoothened_values', 'erc_local_gradient', 'erc_local_mean']\n",
        "\n",
        "  original_previous_day_fire = df_chunk['PrevFireMask']\n",
        "  original_next_day_fire = df_chunk['FireMask']\n",
        "\n",
        "  #general cleaning for classifier and regressor\n",
        "  drop_neg_df = df_chunk[df_chunk['FireMask'] >=0]\n",
        "\n",
        "  #only regressor selection\n",
        "  regressor_target = drop_neg_df['FireMask']\n",
        "\n",
        "  #cleaning specifically for the classifier\n",
        "  classifier_target = np.where(regressor_target > 0, 1, 0)\n",
        "  dropped_chunk = drop_neg_df.drop(labels=['PrevFireMask','FireMask','image_id'], axis=1)\n",
        "  output_chunk = dropped_chunk[col_list]\n",
        "  return output_chunk,regressor_target,classifier_target, original_previous_day_fire, original_next_day_fire\n"
      ],
      "metadata": {
        "id": "wwtDvsYWPRUi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading unsupervised models"
      ],
      "metadata": {
        "id": "uV_6YOSBys_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#standard_scalar_model\n",
        "with open('/content/standard_scalar_model', 'rb') as ss_file:\n",
        "    loaded_scalar_model = pickle.load(ss_file)\n",
        "\n",
        "# pca model chosen:\n",
        "with open('/content/pca_model_8', 'rb') as pca_file:\n",
        "    loaded_pca_model = pickle.load(pca_file)\n",
        "\n",
        "#kmeans model chosen:\n",
        "with open('/content/kmean_model_1', 'rb') as kmean_file:\n",
        "    loaded_kmean_model = pickle.load(kmean_file)"
      ],
      "metadata": {
        "id": "VLblph4zzTNB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setting up data"
      ],
      "metadata": {
        "id": "RrGmDciKECYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "train_df = read_full_csv('train.csv')\n",
        "print('train loaded')\n",
        "train_cleaned_df,train_regressor_target,train_classifier_target, train_original_previous_day_fire, train_original_next_day_fire = cleaner_1(train_df)\n",
        "del train_df\n",
        "gc.collect()\n",
        "\n",
        "print('initializing train data scaling')\n",
        "train_data_scaled = loaded_scalar_model.transform(train_cleaned_df)\n",
        "del train_cleaned_df\n",
        "gc.collect()\n",
        "\n",
        "print('initializing train pca')\n",
        "train_data_pca = loaded_pca_model.transform(train_data_scaled)\n",
        "del train_data_scaled\n",
        "gc.collect()\n",
        "\n",
        "print('getting train cluster labels')\n",
        "train_cluster_labels = loaded_kmean_model.predict(train_data_pca)\n",
        "\n",
        "\n",
        "#evaluation\n",
        "eval_df = read_full_csv('eval.csv')\n",
        "print('eval loaded')\n",
        "eval_cleaned_df,eval_regressor_target,eval_classifier_target, eval_original_previous_day_fire, eval_original_next_day_fire = cleaner_1(eval_df)\n",
        "del eval_df\n",
        "gc.collect()\n",
        "\n",
        "print('initializing eval data scaling')\n",
        "eval_data_scaled = loaded_scalar_model.transform(eval_cleaned_df)\n",
        "del eval_cleaned_df\n",
        "gc.collect()\n",
        "\n",
        "print('initializing eval pca')\n",
        "eval_data_pca = loaded_pca_model.transform(eval_data_scaled)\n",
        "del eval_data_scaled\n",
        "gc.collect()\n",
        "\n",
        "print('getting eval cluster labels')\n",
        "eval_cluster_labels = loaded_kmean_model.predict(eval_data_pca)\n",
        "\n",
        "\n",
        "#test\n",
        "test_df = read_full_csv('test.csv')\n",
        "print('test loaded')\n",
        "test_cleaned_df,test_regressor_target,test_classifier_target, test_original_previous_day_fire, test_original_next_day_fire = cleaner_1(test_df)\n",
        "del test_df\n",
        "gc.collect()\n",
        "\n",
        "print('initializing test data scaling')\n",
        "test_data_scaled = loaded_scalar_model.transform(test_cleaned_df)\n",
        "del test_cleaned_df\n",
        "gc.collect()\n",
        "\n",
        "print('initializing test pca')\n",
        "test_data_pca = loaded_pca_model.transform(test_data_scaled)\n",
        "del test_data_scaled\n",
        "gc.collect()\n",
        "\n",
        "print('getting test cluster labels')\n",
        "test_cluster_labels = loaded_kmean_model.predict(test_data_pca)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODpvZwg0D6p5",
        "outputId": "f59bb2a6-afc2-4e7b-9068-62410ea2f421"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loaded\n",
            "initializing train data scaling\n",
            "initializing train pca\n",
            "getting train cluster labels\n",
            "eval loaded\n",
            "initializing eval data scaling\n",
            "initializing eval pca\n",
            "getting eval cluster labels\n",
            "test loaded\n",
            "initializing test data scaling\n",
            "initializing test pca\n",
            "getting test cluster labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## breaking data into clusters function"
      ],
      "metadata": {
        "id": "fyVJ6d4FONSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def break_cluster(pca_data,cluster_labels,supervised_target):\n",
        "  unique_labels = list(np.unique(cluster_labels))\n",
        "  storage_dict={}\n",
        "  for a_label in unique_labels:\n",
        "    label_index = np.where(cluster_labels == a_label)[0]\n",
        "    feature_data = pca_data[label_index]\n",
        "    target_data = supervised_target[label_index]\n",
        "    yield a_label,feature_data,target_data"
      ],
      "metadata": {
        "id": "bCu311JzNj7y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifier Model building"
      ],
      "metadata": {
        "id": "Gb4ok8QnKmXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic regression"
      ],
      "metadata": {
        "id": "XSAnGRIMKrzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_train(a_label,feature_df,target_array,penalty = ['l2']):\n",
        "  model_counter = 0\n",
        "  hyper_params ={'penalty': penalty}\n",
        "  name_param = {}\n",
        "  for params in ParameterGrid(hyper_params):\n",
        "    print('cluster: ', a_label, ', logistic regression for params: ', params)\n",
        "    model_counter+=1\n",
        "    print('initializing training of logistic regression')\n",
        "    lrs = LogisticRegression(**params)\n",
        "    lrs.fit(feature_df,target_array)\n",
        "    print('training complete')\n",
        "    print('saving model...')\n",
        "    model_name = 'cluster_'+str(a_label)+'_logistic_model_'+str(model_counter)\n",
        "    with open(model_name, 'wb') as model_file:\n",
        "      pickle.dump(lrs, model_file)\n",
        "    print('model saved')\n",
        "    name_param[model_name] = params\n",
        "  return name_param\n"
      ],
      "metadata": {
        "id": "bpn5fQDfKqFl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SGD CLASSIFIER"
      ],
      "metadata": {
        "id": "vZx1fnupua7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd_train(a_label,feature_df,target_array,penalty = ['l1', 'l2', 'elasticnet'],random_state=[0],n_jobs=[-1]):\n",
        "  model_counter = 0\n",
        "  hyper_params ={'penalty': penalty,'random_state':random_state,'n_jobs':n_jobs}\n",
        "  name_param = {}\n",
        "  for params in ParameterGrid(hyper_params):\n",
        "    print('cluster: ', a_label, ', SGD for params: ', params)\n",
        "    model_counter+=1\n",
        "    print('initializing training of SGD ')\n",
        "    sgd = SGDClassifier(**params)\n",
        "    sgd.fit(feature_df,target_array)\n",
        "    print('training complete')\n",
        "    print('saving model...')\n",
        "    model_name = 'cluster_'+str(a_label)+'_SGD_model_'+str(model_counter)\n",
        "    with open(model_name, 'wb') as model_file:\n",
        "      pickle.dump(sgd, model_file)\n",
        "    print('model saved')\n",
        "    name_param[model_name] = params\n",
        "  return name_param\n"
      ],
      "metadata": {
        "id": "MP4ghcs-Udud"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear SVC"
      ],
      "metadata": {
        "id": "E7wKTOOSv_LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_svc_train(a_label,feature_df,target_array,penalty = ['l2'],random_state=[0]):\n",
        "  model_counter = 0\n",
        "  hyper_params ={'penalty': penalty,'random_state':random_state}\n",
        "  name_param = {}\n",
        "  for params in ParameterGrid(hyper_params):\n",
        "    print('cluster: ', a_label, ', linear svc for params: ', params)\n",
        "    model_counter+=1\n",
        "    print('initializing training of svc')\n",
        "    svc = LinearSVC(**params)\n",
        "    svc.fit(feature_df,target_array)\n",
        "    print('training complete')\n",
        "    print('saving model...')\n",
        "    model_name = 'cluster_'+str(a_label)+'_linearSVC_model_'+str(model_counter)\n",
        "    with open(model_name, 'wb') as model_file:\n",
        "      pickle.dump(svc, model_file)\n",
        "    print('model saved')\n",
        "    name_param[model_name] = params\n",
        "  return name_param"
      ],
      "metadata": {
        "id": "5U5UJeFTUeGG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest"
      ],
      "metadata": {
        "id": "5DnktQftzsRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_forest_train(a_label,feature_df,target_array,n_estimators = [100,200,400],min_samples_split = [4096],random_state=[0]):\n",
        "  model_counter = 0\n",
        "  hyper_params ={'n_estimators': n_estimators,'random_state':random_state}\n",
        "  name_param = {}\n",
        "  for params in ParameterGrid(hyper_params):\n",
        "    print('cluster: ', a_label, ', random forest for params: ', params)\n",
        "    model_counter+=1\n",
        "    print('initializing training of random forest')\n",
        "    rfc = RandomForestClassifier(**params)\n",
        "    rfc.fit(feature_df,target_array)\n",
        "    print('training complete')\n",
        "    print('saving model...')\n",
        "    model_name = 'cluster_'+str(a_label)+'_random_forest_model_'+str(model_counter)\n",
        "    with open(model_name, 'wb') as model_file:\n",
        "      pickle.dump(rfc, model_file)\n",
        "    print('model saved')\n",
        "    name_param[model_name] = params\n",
        "  return name_param"
      ],
      "metadata": {
        "id": "EkYv-Wt_yqGR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XGB classifier"
      ],
      "metadata": {
        "id": "8GBDCdxI5TBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xgb_train(a_label,feature_df,target_array,n_estimators = [100,200,400],tree_method =['gpu_hist'] ):\n",
        "  model_counter = 0\n",
        "  hyper_params ={'n_estimators': n_estimators,'tree_method':tree_method}\n",
        "  name_param = {}\n",
        "  for params in ParameterGrid(hyper_params):\n",
        "    print('cluster: ', a_label, ', xgb for params: ', params)\n",
        "    model_counter+=1\n",
        "    print('initializing training of random forest')\n",
        "    xgc = xgb.XGBClassifier(**params)\n",
        "\n",
        "    # optimizing for gpu usage\n",
        "    big_chunks = np.array_split(feature_df, 100)\n",
        "    small_chunks = np.array_split(target_array, 100)\n",
        "    print('chunks starting')\n",
        "    first_fit = True\n",
        "    counter = 0\n",
        "    for feature_chunk, target_chunk in zip(big_chunks,small_chunks):\n",
        "      print(counter)\n",
        "      if first_fit == True:\n",
        "        xgc.fit(feature_chunk,target_chunk)\n",
        "        first_fit = False\n",
        "        print('chunks started')\n",
        "      else:\n",
        "        xgc.fit(feature_chunk,target_chunk, xgb_model = xgc)\n",
        "        counter +=1\n",
        "\n",
        "    print('training complete')\n",
        "    print('saving model...')\n",
        "    model_name = 'cluster_'+str(a_label)+'_xgb_model_'+str(model_counter)\n",
        "    with open(model_name, 'wb') as model_file:\n",
        "      pickle.dump(xgc, model_file)\n",
        "    print('model saved')\n",
        "    name_param[model_name] = params\n",
        "  return name_param"
      ],
      "metadata": {
        "id": "jiLYrj945W0O"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Complement Naive Bayes"
      ],
      "metadata": {
        "id": "uD0Ci-Bj0Ngj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def complement_nb_train(a_label,feature_df,target_array):\n",
        "  name_param = {}\n",
        "  print('cluster: ', a_label, ', Complement for params: ',)\n",
        "  model_counter=1\n",
        "  print('initializing training of ComplementNB')\n",
        "  cnb = GaussianNB()\n",
        "  cnb.fit(feature_df,target_array)\n",
        "  print('training complete')\n",
        "  print('saving model...')\n",
        "  model_name = 'cluster_'+str(a_label)+'_ComplementNB_'+str(model_counter)\n",
        "  with open(model_name, 'wb') as model_file:\n",
        "    pickle.dump(cnb, model_file)\n",
        "  print('model saved')\n",
        "  name_param[model_name] = 'default params'\n",
        "  return name_param"
      ],
      "metadata": {
        "id": "UQTJN5370Ngy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gaussian Naive Bayes"
      ],
      "metadata": {
        "id": "q_8q_Su0173Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_nb_train(a_label,feature_df,target_array):\n",
        "  name_param = {}\n",
        "  print('cluster: ', a_label, ', gaussian for params: ',)\n",
        "  model_counter=1\n",
        "  print('initializing training of guassianNB')\n",
        "  gnb = GaussianNB()\n",
        "  gnb.fit(feature_df,target_array)\n",
        "  print('training complete')\n",
        "  print('saving model...')\n",
        "  model_name = 'cluster_'+str(a_label)+'_guassianNB_'+str(model_counter)\n",
        "  with open(model_name, 'wb') as model_file:\n",
        "    pickle.dump(gnb, model_file)\n",
        "  print('model saved')\n",
        "  name_param[model_name] = 'default params'\n",
        "  return name_param"
      ],
      "metadata": {
        "id": "3i78PXRj173a"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### classifier training"
      ],
      "metadata": {
        "id": "oQpxby377DYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_dict = {}\n",
        "cluster = break_cluster(train_data_pca,train_cluster_labels,train_classifier_target)\n",
        "for label,feature,target in cluster:\n",
        "  print(label, len(feature),len(target))\n",
        "  print(np.unique(target))\n",
        "  print(type(target))\n",
        "  #log_dict = logistic_train(label,feature,target)\n",
        "  #sgd_dict = sgd_train(label,feature,target)\n",
        "  #svc_dict = linear_svc_train(label,feature,target)\n",
        "  #rfc_dict = random_forest_train(label,feature,target)\n",
        "  xgb_dict = xgb_train(label,feature,target)\n",
        "  #cnb_dict = complement_nb_train(label,feature,target)\n",
        "  #gnb_dict = gaussian_nb_train(label,feature,target)\n",
        "  #main_dict[label] = [log_dict,sgd_dict,rfc_dict,xgb_dict,cnb_dict,gnb_dict]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiUwqbfU69jt",
        "outputId": "76747795-147d-450d-fc8d-2f3c8a1f4860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 478301 478301\n",
            "[0 1]\n",
            "<class 'numpy.ndarray'>\n",
            "cluster:  0 , xgb for params:  {'n_estimators': 100, 'tree_method': 'gpu_hist'}\n",
            "initializing training of random forest\n",
            "chunks starting\n",
            "0\n",
            "chunks started\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "found error\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "training complete\n",
            "saving model...\n",
            "model saved\n",
            "cluster:  0 , xgb for params:  {'n_estimators': 200, 'tree_method': 'gpu_hist'}\n",
            "initializing training of random forest\n",
            "chunks starting\n",
            "0\n",
            "chunks started\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "found error\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "training complete\n",
            "saving model...\n",
            "model saved\n",
            "cluster:  0 , xgb for params:  {'n_estimators': 400, 'tree_method': 'gpu_hist'}\n",
            "initializing training of random forest\n",
            "chunks starting\n",
            "0\n",
            "chunks started\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "found error\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "found error\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "training complete\n",
            "saving model...\n",
            "model saved\n",
            "1 24229472 24229472\n",
            "[0 1]\n",
            "<class 'numpy.ndarray'>\n",
            "cluster:  1 , xgb for params:  {'n_estimators': 100, 'tree_method': 'gpu_hist'}\n",
            "initializing training of random forest\n",
            "chunks starting\n",
            "0\n",
            "chunks started\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "found error\n",
            "32\n",
            "33\n",
            "found error\n",
            "33\n",
            "34\n",
            "35\n",
            "found error\n",
            "35\n",
            "36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored on calling ctypes callback function: <bound method DataIter._next_wrapper of <xgboost.data.SingleBatchInternalIter object at 0x7efddb689270>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 500, in _next_wrapper\n",
            "    Initialize data from a datatable Frame.\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37\n",
            "found error\n",
            "37\n",
            "38\n",
            "39\n"
          ]
        }
      ]
    }
  ]
}