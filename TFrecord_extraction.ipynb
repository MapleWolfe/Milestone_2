{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "hD7ndnzBQ2A1"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MapleWolfe/Milestone_2/blob/Jai/TFrecord_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data extraction from TF records"
      ],
      "metadata": {
        "id": "IuuckSotcRHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## installs, imports, pre-sets"
      ],
      "metadata": {
        "id": "Uv1syv3gdvb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpkJJ4VBXvPb",
        "outputId": "2b5555f1-ab09-409e-83bc-d841c0dfca08"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rapidsai-csp-utils'...\n",
            "remote: Enumerating objects: 390, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 390 (delta 89), reused 51 (delta 51), pack-reused 269\u001b[K\n",
            "Receiving objects: 100% (390/390), 107.11 KiB | 8.24 MiB/s, done.\n",
            "Resolving deltas: 100% (191/191), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pynvml\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 4.0 MB/s eta 0:00:00\n",
            "Installing collected packages: pynvml\n",
            "Successfully installed pynvml-11.5.0\n",
            "***********************************************************************\n",
            "Woo! Your instance has the right kind of GPU, a Tesla T4!\n",
            "We will now install RAPIDS cuDF, cuML, and cuGraph via pip! \n",
            "Please stand by, should be quick...\n",
            "***********************************************************************\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.nvidia.com\n",
            "Collecting cudf-cu11\n",
            "  Downloading https://pypi.nvidia.com/cudf-cu11/cudf_cu11-23.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 489.3/489.3 MB 2.7 MB/s eta 0:00:00\n",
            "Collecting cuml-cu11\n",
            "  Downloading https://pypi.nvidia.com/cuml-cu11/cuml_cu11-23.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1079.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 GB 1.6 MB/s eta 0:00:00\n",
            "Collecting cugraph-cu11\n",
            "  Downloading https://pypi.nvidia.com/cugraph-cu11/cugraph_cu11-23.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1160.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 GB 1.4 MB/s eta 0:00:00\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 21.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (5.3.0)\n",
            "Collecting cubinlinker-cu11 (from cudf-cu11)\n",
            "  Downloading https://pypi.nvidia.com/cubinlinker-cu11/cubinlinker_cu11-0.3.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 87.1 MB/s eta 0:00:00\n",
            "Collecting cuda-python<12.0,>=11.7.1 (from cudf-cu11)\n",
            "  Downloading cuda_python-11.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/16.5 MB 45.9 MB/s eta 0:00:00\n",
            "Collecting cupy-cuda11x>=12.0.0 (from cudf-cu11)\n",
            "  Downloading cupy_cuda11x-12.1.0-cp310-cp310-manylinux2014_x86_64.whl (89.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.3/89.3 MB 9.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (2023.4.0)\n",
            "Collecting numba>=0.57 (from cudf-cu11)\n",
            "  Downloading numba-0.57.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 84.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (1.22.4)\n",
            "Collecting nvtx>=0.2.1 (from cudf-cu11)\n",
            "  Downloading nvtx-0.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 428.4/428.4 kB 41.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (23.1)\n",
            "Requirement already satisfied: pandas<1.6.0dev0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (1.5.3)\n",
            "Collecting protobuf<4.22,>=4.21.6 (from cudf-cu11)\n",
            "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 409.8/409.8 kB 40.7 MB/s eta 0:00:00\n",
            "Collecting ptxcompiler-cu11 (from cudf-cu11)\n",
            "  Downloading https://pypi.nvidia.com/ptxcompiler-cu11/ptxcompiler_cu11-0.7.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 89.0 MB/s eta 0:00:00\n",
            "Collecting pyarrow==11.* (from cudf-cu11)\n",
            "  Downloading pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.9/34.9 MB 17.9 MB/s eta 0:00:00\n",
            "Collecting rmm-cu11==23.6.* (from cudf-cu11)\n",
            "  Downloading https://pypi.nvidia.com/rmm-cu11/rmm_cu11-23.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 89.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (4.5.0)\n",
            "Collecting dask-cuda==23.6.* (from cuml-cu11)\n",
            "  Downloading dask_cuda-23.6.0-py3-none-any.whl (125 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.2/125.2 kB 16.8 MB/s eta 0:00:00\n",
            "Collecting dask-cudf-cu11==23.6.* (from cuml-cu11)\n",
            "  Downloading https://pypi.nvidia.com/dask-cudf-cu11/dask_cudf_cu11-23.6.0-py3-none-any.whl (79 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.6/79.6 kB 13.1 MB/s eta 0:00:00\n",
            "Collecting dask==2023.3.2 (from cuml-cu11)\n",
            "  Downloading dask-2023.3.2-py3-none-any.whl (1.2 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 88.3 MB/s eta 0:00:00\n",
            "Collecting distributed==2023.3.2.1 (from cuml-cu11)\n",
            "  Downloading distributed-2023.3.2.1-py3-none-any.whl (957 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 957.1/957.1 kB 77.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from cuml-cu11) (1.2.0)\n",
            "Collecting raft-dask-cu11==23.6.* (from cuml-cu11)\n",
            "  Downloading https://pypi.nvidia.com/raft-dask-cu11/raft_dask_cu11-23.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (214.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 214.3/214.3 MB 5.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from cuml-cu11) (1.10.1)\n",
            "Collecting treelite==3.2.0 (from cuml-cu11)\n",
            "  Downloading treelite-3.2.0-py3-none-manylinux2014_x86_64.whl (1.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 75.6 MB/s eta 0:00:00\n",
            "Collecting treelite-runtime==3.2.0 (from cuml-cu11)\n",
            "  Downloading treelite_runtime-3.2.0-py3-none-manylinux2014_x86_64.whl (198 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 198.2/198.2 kB 27.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from dask==2023.3.2->cuml-cu11) (8.1.3)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from dask==2023.3.2->cuml-cu11) (2.2.1)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask==2023.3.2->cuml-cu11) (1.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask==2023.3.2->cuml-cu11) (6.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from dask==2023.3.2->cuml-cu11) (0.12.0)\n",
            "Collecting importlib-metadata>=4.13.0 (from dask==2023.3.2->cuml-cu11)\n",
            "  Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
            "Collecting pynvml<11.5,>=11.0.0 (from dask-cuda==23.6.*->cuml-cu11)\n",
            "  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.0/47.0 kB 6.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: zict>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from dask-cuda==23.6.*->cuml-cu11) (3.0.0)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.3.2.1->cuml-cu11) (3.1.2)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.3.2.1->cuml-cu11) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.3.2.1->cuml-cu11) (1.0.5)\n",
            "Requirement already satisfied: psutil>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.3.2.1->cuml-cu11) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.3.2.1->cuml-cu11) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.3.2.1->cuml-cu11) (1.7.0)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.3.2.1->cuml-cu11) (6.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.3.2.1->cuml-cu11) (1.26.15)\n",
            "Collecting pylibraft-cu11==23.6.* (from raft-dask-cu11==23.6.*->cuml-cu11)\n",
            "  Downloading https://pypi.nvidia.com/pylibraft-cu11/pylibraft_cu11-23.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.7/471.7 MB 2.8 MB/s eta 0:00:00\n",
            "Collecting ucx-py-cu11==0.32.* (from raft-dask-cu11==23.6.*->cuml-cu11)\n",
            "  Downloading https://pypi.nvidia.com/ucx-py-cu11/ucx_py_cu11-0.32.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/7.9 MB 84.9 MB/s eta 0:00:00\n",
            "Collecting pylibcugraph-cu11==23.6.* (from cugraph-cu11)\n",
            "  Downloading https://pypi.nvidia.com/pylibcugraph-cu11/pylibcugraph_cu11-23.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1159.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 GB 1.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 15.4 MB/s eta 0:00:00\n",
            "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 30.3 MB/s eta 0:00:00\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 20.2 MB/s eta 0:00:00\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python<12.0,>=11.7.1->cudf-cu11) (0.29.34)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda11x>=12.0.0->cudf-cu11) (0.8.1)\n",
            "Collecting llvmlite<0.41,>=0.40.0dev0 (from numba>=0.57->cudf-cu11)\n",
            "  Downloading llvmlite-0.40.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.1/42.1 MB 15.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<1.6.0dev0,>=1.3->cudf-cu11) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<1.6.0dev0,>=1.3->cudf-cu11) (2022.7.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp) (3.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask==2023.3.2->cuml-cu11) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed==2023.3.2.1->cuml-cu11) (2.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<1.6.0dev0,>=1.3->cudf-cu11) (1.16.0)\n",
            "Installing collected packages: ptxcompiler-cu11, nvtx, cubinlinker-cu11, pynvml, pyarrow, protobuf, multidict, llvmlite, importlib-metadata, frozenlist, cupy-cuda11x, cuda-python, async-timeout, yarl, ucx-py-cu11, treelite-runtime, treelite, numba, dask, aiosignal, rmm-cu11, distributed, aiohttp, pylibraft-cu11, dask-cuda, cudf-cu11, raft-dask-cu11, pylibcugraph-cu11, dask-cudf-cu11, cuml-cu11, cugraph-cu11\n",
            "  Attempting uninstall: pynvml\n",
            "    Found existing installation: pynvml 11.5.0\n",
            "    Uninstalling pynvml-11.5.0:\n",
            "      Successfully uninstalled pynvml-11.5.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 9.0.0\n",
            "    Uninstalling pyarrow-9.0.0:\n",
            "      Successfully uninstalled pyarrow-9.0.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.39.1\n",
            "    Uninstalling llvmlite-0.39.1:\n",
            "      Successfully uninstalled llvmlite-0.39.1\n",
            "  Attempting uninstall: cupy-cuda11x\n",
            "    Found existing installation: cupy-cuda11x 11.0.0\n",
            "    Uninstalling cupy-cuda11x-11.0.0:\n",
            "      Successfully uninstalled cupy-cuda11x-11.0.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.56.4\n",
            "    Uninstalling numba-0.56.4:\n",
            "      Successfully uninstalled numba-0.56.4\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2022.12.1\n",
            "    Uninstalling dask-2022.12.1:\n",
            "      Successfully uninstalled dask-2022.12.1\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 2022.12.1\n",
            "    Uninstalling distributed-2022.12.1:\n",
            "      Successfully uninstalled distributed-2022.12.1\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 cubinlinker-cu11-0.3.0.post1 cuda-python-11.8.2 cudf-cu11-23.6.0 cugraph-cu11-23.6.2 cuml-cu11-23.6.0 cupy-cuda11x-12.1.0 dask-2023.3.2 dask-cuda-23.6.0 dask-cudf-cu11-23.6.0 distributed-2023.3.2.1 frozenlist-1.3.3 importlib-metadata-6.7.0 llvmlite-0.40.1 multidict-6.0.4 numba-0.57.1 nvtx-0.2.5 protobuf-4.21.12 ptxcompiler-cu11-0.7.0.post1 pyarrow-11.0.0 pylibcugraph-cu11-23.6.2 pylibraft-cu11-23.6.1 pynvml-11.4.1 raft-dask-cu11-23.6.1 rmm-cu11-23.6.0 treelite-3.2.0 treelite-runtime-3.2.0 ucx-py-cu11-0.32.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cupy-cuda11x in /usr/local/lib/python3.10/dist-packages (12.1.0)\n",
            "Requirement already satisfied: numpy<1.27,>=1.20 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda11x) (1.22.4)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda11x) (0.8.1)\n",
            "\n",
            "          ***********************************************************************\n",
            "          The pip install of RAPIDS is complete.\n",
            "          \n",
            "          Please do not run any further installation from the conda based installation methods, as they may cause issues!  \n",
            "          \n",
            "          Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts. \n",
            "r          \n",
            "          Troubleshooting:\n",
            "             - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files. \n",
            "             - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n",
            "          ***********************************************************************\n",
            "          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_fG6RA56XD_4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import skimage\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import cudf\n",
        "import cupy as cp\n",
        "import gc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading TF records from google drive"
      ],
      "metadata": {
        "id": "B_yF4lBdd2_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's mount the drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# let's look into the zip file stored in the google drive\n",
        "wild_fire_file_path = '/content/drive/MyDrive/next_day_wildfire.zip'\n",
        "wildfire_zip =  zipfile.ZipFile(wild_fire_file_path, 'r')\n",
        "tf_record_file_names = wildfire_zip.namelist()\n",
        "\n",
        "print('number of TF records:', len(tf_record_file_names))\n",
        "print('file names of tf records within the zip:')\n",
        "print(tf_record_file_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrCeIqHYul-S",
        "outputId": "2cc99179-f62e-45dd-c22d-588ccc18465b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "number of TF records: 19\n",
            "file names of tf records within the zip:\n",
            "['next_day_wildfire_spread_eval_00.tfrecord', 'next_day_wildfire_spread_eval_01.tfrecord', 'next_day_wildfire_spread_test_00.tfrecord', 'next_day_wildfire_spread_test_01.tfrecord', 'next_day_wildfire_spread_train_00.tfrecord', 'next_day_wildfire_spread_train_01.tfrecord', 'next_day_wildfire_spread_train_02.tfrecord', 'next_day_wildfire_spread_train_03.tfrecord', 'next_day_wildfire_spread_train_04.tfrecord', 'next_day_wildfire_spread_train_05.tfrecord', 'next_day_wildfire_spread_train_06.tfrecord', 'next_day_wildfire_spread_train_07.tfrecord', 'next_day_wildfire_spread_train_08.tfrecord', 'next_day_wildfire_spread_train_09.tfrecord', 'next_day_wildfire_spread_train_10.tfrecord', 'next_day_wildfire_spread_train_11.tfrecord', 'next_day_wildfire_spread_train_12.tfrecord', 'next_day_wildfire_spread_train_13.tfrecord', 'next_day_wildfire_spread_train_14.tfrecord']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzipping one file at a time\n",
        "def one_file_unzip(tf_record_file_name, zipfile_variable):\n",
        "  extracted_record_path = zipfile_variable.extract(tf_record_file_name)\n",
        "  raw_dataset = tf.data.TFRecordDataset(extracted_record_path)\n",
        "  return raw_dataset\n",
        "\n",
        "# yielding out one record at a time\n",
        "def extract_one_row(tf_record_dataset):\n",
        "  for i, raw_record in enumerate(tf_record_dataset.take(tf_record_dataset.cardinality().numpy())):\n",
        "    one_record_dict = {}\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(raw_record.numpy())\n",
        "\n",
        "    for key, feature in example.features.feature.items():\n",
        "\n",
        "      kind = feature.WhichOneof('kind')\n",
        "      one_record_dict[key] = np.array(getattr(feature, kind).value).reshape(64,64)\n",
        "    yield one_record_dict"
      ],
      "metadata": {
        "id": "QuxrZXk6cQEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## let's create features from all images"
      ],
      "metadata": {
        "id": "XErZ3dz7Hs19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### feature description given by dataset maker"
      ],
      "metadata": {
        "id": "hD7ndnzBQ2A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data variables\n",
        "\n",
        "INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph',\n",
        "                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n",
        "\n",
        "OUTPUT_FEATURES = ['FireMask']\n",
        "\n",
        "\n",
        "# underlying feature value ranges:\n",
        "# (min_clip, max_clip, mean, standard deviation)\n",
        "\n",
        "feature_description_dict = {\n",
        "    # Elevation in m: between 0.1 percentile and 99.9 percentile\n",
        "    'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n",
        "\n",
        "    # Palmer Drought Severity Index: between 0.1 percentile and 99.9 percentile\n",
        "    'pdsi': (-6.12974870967865, 7.876040384292651, -0.0052714925, 2.6823447),\n",
        "\n",
        "    #Vegetation index times 10,000: between -1 and 1\n",
        "    'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),\n",
        "\n",
        "    # Precipitation in mm: between 0.0 and 99.9 percentile\n",
        "    'pr': (0.0, 44.53038024902344, 1.7398051, 4.482833),\n",
        "\n",
        "    # Specific humidity: between 0 and 1\n",
        "    'sph': (0., 1., 0.0071658953, 0.0042835088),\n",
        "\n",
        "    # Wind direction in degrees clockwise from north: between 0 and 360.\n",
        "    'th': (0., 360.0, 190.32976, 72.59854),\n",
        "\n",
        "    #Min temp: between 253.15 kelvin and 99.9 percentile\n",
        "    'tmmn': (253.15, 298.94891357421875, 281.08768, 8.982386),\n",
        "\n",
        "    #Max temp: between 253.15 kelvin and 99.9 percentile\n",
        "    'tmmx': (253.15, 315.09228515625, 295.17383, 9.815496),\n",
        "\n",
        "    # Wind speed in m/s: between 0. and 99.9 percentile\n",
        "    'vs': (0.0, 10.024310074806237, 3.8500874, 1.4109988),\n",
        "\n",
        "    # NFDRS fire danger index energy release component BTU's per square foot.\n",
        "    # 0., 99.9 percentile\n",
        "    'erc': (0.0, 106.24891662597656, 37.326267, 20.846027),\n",
        "\n",
        "    # Population density: between 0 and 99.9 percentile\n",
        "    'population': (0., 2534.06298828125, 25.531384, 154.72331),\n",
        "\n",
        "    # We don't want to normalize the FireMasks.\n",
        "    # 1 indicates fire, 0 no fire, -1 unlabeled data\n",
        "    'PrevFireMask': (-1., 1., 0., 1.),\n",
        "    'FireMask': (-1., 1., 0., 1.)\n",
        "}\n"
      ],
      "metadata": {
        "id": "E4-Gd5OVc94v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature generation"
      ],
      "metadata": {
        "id": "eIZ_dwWPQlRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets define the min max scaling function\n",
        "def min_max_scaling(array,min_val,max_val):\n",
        "    scaled_array = np.clip((array - min_val) / (max_val - min_val), 0, 1)\n",
        "    return scaled_array\n",
        "\n",
        "# let's apply guassian smoothing\n",
        "def gaussian_smoothing(image_array,sigma_val):\n",
        "  smooth_array = skimage.filters.gaussian(image_array, sigma=1)\n",
        "  return smooth_array\n",
        "\n",
        "#lets get the rate of change and mean,\n",
        "def local_pixel_features(image_array,radius_val):\n",
        "  footprint = skimage.morphology.disk(radius_val)\n",
        "  gradient_array = skimage.filters.rank.gradient(image_array, footprint)\n",
        "  mean_array = skimage.filters.rank.mean(image_array, footprint)\n",
        "  return gradient_array,mean_array\n",
        "\n",
        "#use altitude edge to identify whether pixel is at a similar altitude as any pixel that has fire\n",
        "def fire_pixel_shared_altitude(row_dict, normalized_array, previous_day_fire = 'PrevFireMask'):\n",
        "  edges_array = skimage.feature.canny(normalized_array)\n",
        "  inverted_edges_array = np.logical_not(edges_array).astype(int)\n",
        "  edge_label_array = skimage.measure.label(inverted_edges_array)\n",
        "\n",
        "  previous_fire = row_dict[previous_day_fire]\n",
        "  fire_edge_labels = (edge_label_array*previous_fire)\n",
        "\n",
        "  unique_regions_with_fire = np.unique(fire_edge_labels.flatten())\n",
        "  non_zero_unique_regions = unique_regions_with_fire[unique_regions_with_fire != 0]\n",
        "\n",
        "  fire_at_same_altitude = np.isin(edge_label_array, non_zero_unique_regions).astype(int)\n",
        "  return fire_at_same_altitude\n",
        "\n",
        "def distance_to_fire(row_dict,feature):\n",
        "  # we need to clip the fire mask to account for -1 values (missing values where the satellite was unable to get a clear image)\n",
        "  # for now we take them as no fire objects, however we will not be accounting for these pixels in our model.\n",
        "  fire_mask_array = row_dict[feature].clip(0,1)\n",
        "  inverted_mask_array = 1 - fire_mask_array\n",
        "  distance_transform_array = distance_transform_edt(inverted_mask_array)\n",
        "  return distance_transform_array\n"
      ],
      "metadata": {
        "id": "i7krqq3gVEpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's apply it on all features\n",
        "def build_features(record_dict,min_max_dict,sigma_val,radius_val):\n",
        "  feature_list = record_dict.keys()\n",
        "  output_feature_dict = {}\n",
        "  for a_feature in feature_list:\n",
        "    if a_feature not in ['PrevFireMask','FireMask']:\n",
        "     #min max scaling\n",
        "     feature_min = min_max_dict[a_feature][0]\n",
        "     feature_max = min_max_dict[a_feature][1]\n",
        "     scaled_array = min_max_scaling(record_dict[a_feature],feature_min,feature_max)\n",
        "     #guassian smoothing\n",
        "     smoothen_array = gaussian_smoothing(scaled_array,sigma_val)\n",
        "\n",
        "     #local pixel values: gradient values(rate of change), local mean val.\n",
        "     gradient_array,mean_array = local_pixel_features(smoothen_array,radius_val)\n",
        "\n",
        "     #lets now add these features to our output:\n",
        "     output_feature_dict[a_feature+'_'+'scaled_smoothened_values'] = smoothen_array.flatten()\n",
        "     output_feature_dict[a_feature+'_'+'local_gradient'] = gradient_array.flatten()\n",
        "     output_feature_dict[a_feature+'_'+'local_mean'] = mean_array.flatten()\n",
        "\n",
        "     #lets label pixels if they are at the same elevation (to account for cliffs/mountains/chasms) as the fire\n",
        "     # here we aren't using smoothened array\n",
        "    if a_feature == 'elevation':\n",
        "      fire_at_altitude_array = fire_pixel_shared_altitude(record_dict, scaled_array)\n",
        "      output_feature_dict['fire_at_similar_altitude'] = fire_at_altitude_array.flatten()\n",
        "     #lets move are features into a dict.\n",
        "\n",
        "    # get pixel eucledian distance from fire\n",
        "    if a_feature == 'PrevFireMask':\n",
        "      distance_array = distance_to_fire(record_dict,a_feature)\n",
        "      output_feature_dict['PrevFireMask'] = record_dict[a_feature].flatten()\n",
        "      output_feature_dict['distance_from_fire'] = distance_array.flatten()\n",
        "\n",
        "    if a_feature == 'FireMask':\n",
        "      output_feature_dict['FireMask'] = record_dict[a_feature].flatten()\n",
        "\n",
        "  return output_feature_dict\n",
        "\n"
      ],
      "metadata": {
        "id": "ZJKMGL66HcCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_df_save_csv(string_file_name,tf_record_names = tf_record_file_names, main_zip_file = wildfire_zip, feature_descriptions= feature_description_dict):\n",
        "  first_write = True\n",
        "  total_rows = 0\n",
        "  image_id = 0\n",
        "  large_df_list = []\n",
        "  column_names = []\n",
        "  csv_file_name = string_file_name + '.csv'\n",
        "  for a_tf_record in tf_record_names:\n",
        "    if string_file_name in a_tf_record:\n",
        "      print('started tf record: ', a_tf_record)\n",
        "      raw_dataset = one_file_unzip(a_tf_record, main_zip_file)\n",
        "      row_extraction_generator = extract_one_row(raw_dataset)\n",
        "      single_record_list = []\n",
        "      image_count = 0\n",
        "\n",
        "      for a_row in row_extraction_generator:\n",
        "        all_features_dict_array = build_features(a_row,feature_descriptions,sigma_val=1,radius_val=3)\n",
        "        column_names = all_features_dict_array.keys()\n",
        "        image_id += 1\n",
        "        image_count +=1\n",
        "        image_number_array = np.full(4096, image_id)\n",
        "        all_features_dict_array['image_id'] = image_number_array\n",
        "\n",
        "        if image_count == 1:\n",
        "          all_features_dataframe = cudf.DataFrame.from_dict(all_features_dict_array)\n",
        "        else:\n",
        "          single_row_df = cudf.DataFrame.from_dict(all_features_dict_array)\n",
        "          all_features_dataframe = all_features_dataframe.append(single_row_df, ignore_index=True)\n",
        "\n",
        "        if image_count >= 200:\n",
        "          single_record_list.append(all_features_dataframe)\n",
        "          image_count = 0\n",
        "\n",
        "      if image_count % 200 != 0:\n",
        "        single_record_list.append(all_features_dataframe)\n",
        "        image_count = 0\n",
        "\n",
        "      big_df = cudf.concat(single_record_list, ignore_index=True)\n",
        "      pandas_big_df = big_df.to_pandas()\n",
        "      total_rows += len(pandas_big_df)\n",
        "      if first_write == True:\n",
        "        pandas_big_df.to_csv(csv_file_name, mode='a', index=False, header=True)\n",
        "        first_write = False\n",
        "      else:\n",
        "        pandas_big_df.to_csv(csv_file_name, mode='a', index=False, header=False)\n",
        "\n",
        "        print('completed: ', a_tf_record)\n",
        "\n",
        "  print('csv output is complete')\n",
        "  print('output csv lenght',total_rows)\n",
        "  print('output csv image count: ', total_rows/4096)\n",
        "  print('number of expected images', image_id)\n",
        "  return None"
      ],
      "metadata": {
        "id": "J6trc2M_ooIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print('we are making eval csv')\n",
        "make_df_save_csv('eval')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Duabn-Fvr0T",
        "outputId": "8b49ef0a-2d38-45e7-81ab-84b0cc492358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we are making eval csv\n",
            "started tf record:  next_day_wildfire_spread_eval_00.tfrecord\n",
            "completed:  next_day_wildfire_spread_eval_00.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_eval_01.tfrecord\n",
            "completed:  next_day_wildfire_spread_eval_01.tfrecord\n",
            "csv output is complete\n",
            "output csv lenght 7688192\n",
            "output csv image count:  1877.0\n",
            "number of expected images 1877\n",
            "CPU times: user 5min 25s, sys: 17.8 s, total: 5min 43s\n",
            "Wall time: 6min 11s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print('we are making eval csv')\n",
        "make_df_save_csv('test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS3TsSvmRPU-",
        "outputId": "340c3e80-226e-4fdc-f468-8084b2473e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we are making eval csv\n",
            "started tf record:  next_day_wildfire_spread_test_00.tfrecord\n",
            "completed:  next_day_wildfire_spread_test_00.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_test_01.tfrecord\n",
            "completed:  next_day_wildfire_spread_test_01.tfrecord\n",
            "csv output is complete\n",
            "output csv lenght 6918144\n",
            "output csv image count:  1689.0\n",
            "number of expected images 1689\n",
            "CPU times: user 4min 55s, sys: 14.7 s, total: 5min 9s\n",
            "Wall time: 5min 12s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print('we are making eval csv')\n",
        "make_df_save_csv('train')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu3jlgVMRPmy",
        "outputId": "9c3a6a24-5638-4a5b-d24a-c04477d198ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we are making eval csv\n",
            "started tf record:  next_day_wildfire_spread_train_00.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_00.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_01.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_01.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_02.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_02.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_03.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_03.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_04.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_04.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_05.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_05.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_06.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_06.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_07.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_07.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_08.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_08.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_09.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_09.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_10.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_10.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_11.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_11.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_12.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_12.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_13.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_13.tfrecord\n",
            "started tf record:  next_day_wildfire_spread_train_14.tfrecord\n",
            "completed:  next_day_wildfire_spread_train_14.tfrecord\n",
            "csv output is complete\n",
            "output csv lenght 61353984\n",
            "output csv image count:  14979.0\n",
            "number of expected images 14979\n",
            "CPU times: user 44min 20s, sys: 2min 16s, total: 46min 36s\n",
            "Wall time: 47min 29s\n"
          ]
        }
      ]
    }
  ]
}