{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSDiTkK/29/5y04OueRPl7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MapleWolfe/Milestone_2/blob/ning/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n"
      ],
      "metadata": {
        "id": "1AtvQO3okeAg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy tensorflow pickle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exdqCkghLYiy",
        "outputId": "fcfd57d9-68e3-4e91-fa3a-5785c22dd753"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wild_fire_file_path = '/content/gdrive/MyDrive/Kaggle/next-day-wildfire-spread.zip'\n",
        "wildfire_zip =  zipfile.ZipFile(wild_fire_file_path, 'r')\n",
        "tf_record_file_names = wildfire_zip.namelist()\n",
        "tf_record_file_names\n"
      ],
      "metadata": {
        "id": "QJnF7u2dlXBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import time\n",
        "import math\n",
        "\n",
        "\n",
        "#from include.data import get_data_set\n",
        "from include.model import model, lr\n",
        "\n",
        "# need to fill the train and test x and y\n",
        "train_x =\n",
        "train_y =\n",
        "test_x =\n",
        "test_y =\n",
        "tf.set_random_seed(21)\n",
        "x, y, output, y_pred_cls, global_step, learning_rate = model()\n",
        "global_accuracy = 0\n",
        "epoch_start = 0\n",
        "\n",
        "\n",
        "# PARAMS\n",
        "_BATCH_SIZE = 128\n",
        "_EPOCH = 60\n",
        "_SAVE_PATH = \"./tensorboard/cifar-10-v1.0.0/\"\n",
        "\n",
        "\n",
        "# LOSS AND OPTIMIZER\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "                                   beta1=0.9,\n",
        "                                   beta2=0.999,\n",
        "                                   epsilon=1e-08).minimize(loss, global_step=global_step)\n",
        "\n",
        "\n",
        "# PREDICTION AND ACCURACY CALCULATION\n",
        "correct_prediction = tf.equal(y_pred_cls, tf.argmax(y, axis=1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "# SAVER\n",
        "merged = tf.summary.merge_all()\n",
        "saver = tf.train.Saver()\n",
        "sess = tf.Session()\n",
        "train_writer = tf.summary.FileWriter(_SAVE_PATH, sess.graph)\n",
        "\n",
        "\n",
        "try:\n",
        "    print(\"\n",
        "Trying to restore last checkpoint ...\")\n",
        "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=_SAVE_PATH)\n",
        "    saver.restore(sess, save_path=last_chk_path)\n",
        "    print(\"Restored checkpoint from:\", last_chk_path)\n",
        "except ValueError:\n",
        "    print(\"\n",
        "Failed to restore checkpoint. Initializing variables instead.\")\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    global epoch_start\n",
        "    epoch_start = time()\n",
        "    batch_size = int(math.ceil(len(train_x) / _BATCH_SIZE))\n",
        "    i_global = 0\n",
        "\n",
        "    for s in range(batch_size):\n",
        "        batch_xs = train_x[s*_BATCH_SIZE: (s+1)*_BATCH_SIZE]\n",
        "        batch_ys = train_y[s*_BATCH_SIZE: (s+1)*_BATCH_SIZE]\n",
        "\n",
        "        start_time = time()\n",
        "        i_global, _, batch_loss, batch_acc = sess.run(\n",
        "            [global_step, optimizer, loss, accuracy],\n",
        "            feed_dict={x: batch_xs, y: batch_ys, learning_rate: lr(epoch)})\n",
        "        duration = time() - start_time\n",
        "\n",
        "        if s % 10 == 0:\n",
        "            percentage = int(round((s/batch_size)*100))\n",
        "\n",
        "            bar_len = 29\n",
        "            filled_len = int((bar_len*int(percentage))/100)\n",
        "            bar = '=' * filled_len + '>' + '-' * (bar_len - filled_len)\n",
        "\n",
        "            msg = \"Global step: {:>5} - [{}] {:>3}% - acc: {:.4f} - loss: {:.4f} - {:.1f} sample/sec\"\n",
        "            print(msg.format(i_global, bar, percentage, batch_acc, batch_loss, _BATCH_SIZE / duration))\n",
        "\n",
        "    test_and_save(i_global, epoch)\n",
        "\n",
        "\n",
        "def test_and_save(_global_step, epoch):\n",
        "    global global_accuracy\n",
        "    global epoch_start\n",
        "\n",
        "    i = 0\n",
        "    predicted_class = np.zeros(shape=len(test_x), dtype=np.int)\n",
        "    while i < len(test_x): j = min(i + _BATCH_SIZE, len(test_x)) batch_xs = test_x[i:j, :] batch_ys = test_y[i:j, :] predicted_class[i:j] = sess.run( y_pred_cls, feed_dict={x: batch_xs, y: batch_ys, learning_rate: lr(epoch)} ) i = j correct = (np.argmax(test_y, axis=1) == predicted_class) acc = correct.mean()*100 correct_numbers = correct.sum() hours, rem = divmod(time() - epoch_start, 3600) minutes, seconds = divmod(rem, 60) mes = \" Epoch {} - accuracy: {:.2f}% ({}/{}) - time: {:0>2}:{:0>2}:{:05.2f}\"\n",
        "    print(mes.format((epoch+1), acc, correct_numbers, len(test_x), int(hours), int(minutes), seconds))\n",
        "\n",
        "    if global_accuracy != 0 and global_accuracy < acc: summary = tf.Summary(value=[ tf.Summary.Value(tag=\"Accuracy/test\", simple_value=acc), ]) train_writer.add_summary(summary, _global_step) saver.save(sess, save_path=_SAVE_PATH, global_step=_global_step) mes = \"This epoch receive better accuracy: {:.2f} > {:.2f}. Saving session...\"\n",
        "        print(mes.format(acc, global_accuracy))\n",
        "        global_accuracy = acc\n",
        "\n",
        "    elif global_accuracy == 0:\n",
        "        global_accuracy = acc\n",
        "\n",
        "    print(\"###########################################################################################################\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    train_start = time()\n",
        "\n",
        "    for i in range(_EPOCH):\n",
        "        print(\"\n",
        "Epoch: {}/{}\n",
        "\".format((i+1), _EPOCH))\n",
        "        train(i)\n",
        "\n",
        "    hours, rem = divmod(time() - train_start, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    mes = \"Best accuracy pre session: {:.2f}, time: {:0>2}:{:0>2}:{:05.2f}\"\n",
        "    print(mes.format(global_accuracy, int(hours), int(minutes), seconds))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "sess.close()"
      ],
      "metadata": {
        "id": "iYhVB_rELYlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run Network on Test DataSet:\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from include.data import get_data_set\n",
        "from include.model import model\n",
        "\n",
        "\n",
        "test_x, test_y = get_data_set(\"test\")\n",
        "x, y, output, y_pred_cls, global_step, learning_rate = model()\n",
        "\n",
        "\n",
        "_BATCH_SIZE = 128\n",
        "_CLASS_SIZE = 10\n",
        "_SAVE_PATH = \"./tensorboard/cifar-10-v1.0.0/\"\n",
        "\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "sess = tf.Session()\n",
        "\n",
        "\n",
        "try:\n",
        "    print(\"\n",
        "Trying to restore last checkpoint ...\")\n",
        "    last_chk_path = tf.train.latest_checkpoint(checkpoint_dir=_SAVE_PATH)\n",
        "    saver.restore(sess, save_path=last_chk_path)\n",
        "    print(\"Restored checkpoint from:\", last_chk_path)\n",
        "except ValueError:\n",
        "    print(\"\n",
        "Failed to restore checkpoint. Initializing variables instead.\")\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "def main():\n",
        "    i = 0\n",
        "    predicted_class = np.zeros(shape=len(test_x), dtype=np.int)\n",
        "    while i < len(test_x):\n",
        "        j = min(i + _BATCH_SIZE, len(test_x))\n",
        "        batch_xs = test_x[i:j, :]\n",
        "        batch_ys = test_y[i:j, :]\n",
        "        predicted_class[i:j] = sess.run(y_pred_cls, feed_dict={x: batch_xs, y: batch_ys})\n",
        "        i = j\n",
        "\n",
        "    correct = (np.argmax(test_y, axis=1) == predicted_class)\n",
        "    acc = correct.mean() * 100\n",
        "    correct_numbers = correct.sum()\n",
        "    print()\n",
        "    print(\"Accuracy on Test-Set: {0:.2f}% ({1} / {2})\".format(acc, correct_numbers, len(test_x)))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "sess.close()"
      ],
      "metadata": {
        "id": "kkuDfpSMLYnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uTMKq5jvLYp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8M-UCfzgLYsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IwFXaTsPLYuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RBEDSaxTLYxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W94mgdcwLY0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W2FCyvdJLY2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Bd0Mzu3LY5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tTjj64_nLY7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zwemw1lGLY-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wr-cjZELZAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vGT6r69mLZC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WohGuar5LZFj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}