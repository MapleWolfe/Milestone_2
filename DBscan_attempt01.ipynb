{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4z9ZY8bH16AzPmkedgpUM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MapleWolfe/Milestone_2/blob/main/DBscan_attempt01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## installs, imports, pre-sets"
      ],
      "metadata": {
        "id": "Uv1syv3gdvb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n",
        "!pip install google-cloud-storage"
      ],
      "metadata": {
        "id": "qudk0RcgDLyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_fG6RA56XD_4"
      },
      "outputs": [],
      "source": [
        "#google import options\n",
        "#from google.colab import drive\n",
        "from google.cloud import storage\n",
        "\n",
        "#general usage imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import gc\n",
        "import os\n",
        "import multiprocessing\n",
        "import pickle\n",
        "import json\n",
        "import joblib\n",
        "\n",
        "#model operations imports\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#sklearn classifiers\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#GPU imports\n",
        "import cudf\n",
        "from cuml.naive_bayes import GaussianNB\n",
        "from cuml.naive_bayes import ComplementNB\n",
        "from cuml import LogisticRegression\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "from cuml.dask.cluster import DBSCAN\n",
        "\n",
        "import cupy\n",
        "import xgboost as xgb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y3C-aDrhWqmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GCP set up"
      ],
      "metadata": {
        "id": "TnpY5qVHuE9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/organic-reef-390716-609989a4c6da.json'\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket('fire_train_eval_test_bucket')\n",
        "blob = bucket.blob('test.csv')\n",
        "blob.download_to_filename('test.csv')\n",
        "blob = bucket.blob('eval.csv')\n",
        "blob.download_to_filename('eval.csv')\n",
        "blob = bucket.blob('train.csv')\n",
        "blob.download_to_filename('train.csv')"
      ],
      "metadata": {
        "id": "Wi1hP1Hbs7sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading unsupervised models"
      ],
      "metadata": {
        "id": "uV_6YOSBys_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#standard_scalar_model\n",
        "with open('/content/standard_scalar_model', 'rb') as ss_file:\n",
        "    loaded_scalar_model = pickle.load(ss_file)\n",
        "\n",
        "# pca model chosen:\n",
        "with open('/content/pca_model_8', 'rb') as pca_file:\n",
        "    loaded_pca_model = pickle.load(pca_file)\n",
        "\n",
        "#kmeans model chosen:\n",
        "with open('/content/kmean_model_1', 'rb') as kmean_file:\n",
        "    loaded_kmean_model = pickle.load(kmean_file)"
      ],
      "metadata": {
        "id": "VLblph4zzTNB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to load and clean csv chunks"
      ],
      "metadata": {
        "id": "B_yF4lBdd2_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remember to add .csv at the end of file name\n",
        "def read_csv_in_chunks(file_name,number_images):\n",
        "\n",
        "  #number of rows per image:\n",
        "  pixels_count = 64*64\n",
        "\n",
        "  #upto 200 images at a time\n",
        "  size = number_images*pixels_count\n",
        "\n",
        "  #file string and location for Google cloud storage\n",
        "  file_string = '/content/' + file_name\n",
        "  return pd.read_csv(file_string, chunksize=size)\n",
        "\n",
        "def read_full_csv(file_name):\n",
        "\n",
        "  #file string and location for Google cloud storage\n",
        "  file_string = '/content/' + file_name\n",
        "\n",
        "\n",
        "  return pd.read_csv(file_string)\n",
        "\n",
        "#this is procedure that cleans the data.\n",
        "# cleaner_1 drops all negative \"firemask\" values and converts all values above 0 to 1\n",
        "def cleaner_1(df_chunk):\n",
        "  col_list = ['NDVI_scaled_smoothened_values', 'NDVI_local_gradient', 'NDVI_local_mean', 'tmmn_scaled_smoothened_values', 'tmmn_local_gradient', 'tmmn_local_mean', 'elevation_scaled_smoothened_values', 'elevation_local_gradient', 'elevation_local_mean', 'fire_at_similar_altitude', 'population_scaled_smoothened_values', 'population_local_gradient', 'population_local_mean', 'vs_scaled_smoothened_values', 'vs_local_gradient', 'vs_local_mean', 'pdsi_scaled_smoothened_values', 'pdsi_local_gradient', 'pdsi_local_mean', 'pr_scaled_smoothened_values', 'pr_local_gradient', 'pr_local_mean', 'tmmx_scaled_smoothened_values', 'tmmx_local_gradient', 'tmmx_local_mean', 'sph_scaled_smoothened_values', 'sph_local_gradient', 'sph_local_mean', 'th_scaled_smoothened_values', 'th_local_gradient', 'th_local_mean', 'distance_from_fire', 'erc_scaled_smoothened_values', 'erc_local_gradient', 'erc_local_mean']\n",
        "\n",
        "  original_previous_day_fire = df_chunk['PrevFireMask']\n",
        "  original_next_day_fire = df_chunk['FireMask']\n",
        "\n",
        "  #general cleaning for classifier and regressor\n",
        "  drop_neg_df = df_chunk[df_chunk['FireMask'] >=0]\n",
        "\n",
        "  #only regressor selection\n",
        "  regressor_target = drop_neg_df['FireMask']\n",
        "\n",
        "  #cleaning specifically for the classifier\n",
        "  classifier_target = np.where(regressor_target > 0, 1, 0)\n",
        "  dropped_chunk = drop_neg_df.drop(labels=['PrevFireMask','FireMask','image_id'], axis=1)\n",
        "  output_chunk = dropped_chunk[col_list]\n",
        "  return output_chunk,regressor_target,classifier_target, original_previous_day_fire, original_next_day_fire\n"
      ],
      "metadata": {
        "id": "wwtDvsYWPRUi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setting up data"
      ],
      "metadata": {
        "id": "RrGmDciKECYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "train_df = read_full_csv('train.csv')\n",
        "print('train loaded')\n",
        "train_cleaned_df,train_regressor_target,train_classifier_target, train_original_previous_day_fire, train_original_next_day_fire = cleaner_1(train_df)\n",
        "del train_df\n",
        "gc.collect()\n",
        "\n",
        "print('initializing train data scaling')\n",
        "train_data_scaled = loaded_scalar_model.transform(train_cleaned_df)\n",
        "del train_cleaned_df\n",
        "gc.collect()\n",
        "\n",
        "print('initializing train pca')\n",
        "train_data_pca = loaded_pca_model.transform(train_data_scaled)\n",
        "del train_data_scaled\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "#evaluation\n",
        "eval_df = read_full_csv('eval.csv')\n",
        "print('eval loaded')\n",
        "eval_cleaned_df,eval_regressor_target,eval_classifier_target, eval_original_previous_day_fire, eval_original_next_day_fire = cleaner_1(eval_df)\n",
        "del eval_df\n",
        "gc.collect()\n",
        "\n",
        "print('initializing eval data scaling')\n",
        "eval_data_scaled = loaded_scalar_model.transform(eval_cleaned_df)\n",
        "del eval_cleaned_df\n",
        "gc.collect()\n",
        "\n",
        "print('initializing eval pca')\n",
        "eval_data_pca = loaded_pca_model.transform(eval_data_scaled)\n",
        "del eval_data_scaled\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "#test\n",
        "test_df = read_full_csv('test.csv')\n",
        "print('test loaded')\n",
        "test_cleaned_df,test_regressor_target,test_classifier_target, test_original_previous_day_fire, test_original_next_day_fire = cleaner_1(test_df)\n",
        "del test_df\n",
        "gc.collect()\n",
        "\n",
        "print('initializing test data scaling')\n",
        "test_data_scaled = loaded_scalar_model.transform(test_cleaned_df)\n",
        "del test_cleaned_df\n",
        "gc.collect()\n",
        "\n",
        "print('initializing test pca')\n",
        "test_data_pca = loaded_pca_model.transform(test_data_scaled)\n",
        "del test_data_scaled\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODpvZwg0D6p5",
        "outputId": "67b68b6f-e572-49c5-8055-55b8edee9157"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loaded\n",
            "initializing train data scaling\n",
            "initializing train pca\n",
            "eval loaded\n",
            "initializing eval data scaling\n",
            "initializing eval pca\n",
            "test loaded\n",
            "initializing test data scaling\n",
            "initializing test pca\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBSCAN"
      ],
      "metadata": {
        "id": "Cm-g5fqYFhoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " DBSCAN(min_samples = 2048000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "zqao146pFq9r",
        "outputId": "0d0ccb78-ce12-433b-e6cd-97244532cf21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-874c342b0cb6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_pca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cuml/internals/api_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mprocess_return\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32magglomerative.pyx\u001b[0m in \u001b[0;36mcuml.cluster.agglomerative.AgglomerativeClustering.fit\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mMemoryError\u001b[0m: std::bad_alloc: out_of_memory: CUDA error at: /__w/rmm/rmm/include/rmm/mr/device/cuda_memory_resource.hpp:70: cudaErrorMemoryAllocation out of memory"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HDBSCAN"
      ],
      "metadata": {
        "id": "3DQ-lrdxFvC-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "of9bsepsFvC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agglomerative clustering"
      ],
      "metadata": {
        "id": "c0qb323cFvgz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RyD0scn1Fvgz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}