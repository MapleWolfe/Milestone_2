{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencoder method CNN \n",
    "#reference paper and try other \n",
    "\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from typing import Dict, List, Optional, Text, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern_train = 'next_day_wildfire_spread_train*'\n",
    "file_pattern_test = 'next_day_wildfire_spread_test*'\n",
    "file_pattern_eval = 'next_day_wildfire_spread_eval*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data variables\n",
    "\n",
    "INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph',\n",
    "                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n",
    "\n",
    "OUTPUT_FEATURES = ['FireMask']\n",
    "\n",
    "\n",
    "# underlying feature value ranges:\n",
    "# (min_clip, max_clip, mean, standard deviation)\n",
    "\n",
    "feature_description_dict = {\n",
    "    # Elevation in m: between 0.1 percentile and 99.9 percentile\n",
    "    'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n",
    "\n",
    "    # Palmer Drought Severity Index: between 0.1 percentile and 99.9 percentile\n",
    "    'pdsi': (-6.12974870967865, 7.876040384292651, -0.0052714925, 2.6823447),\n",
    "\n",
    "    #Vegetation index times 10,000: between -1 and 1\n",
    "    'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),\n",
    "\n",
    "    # Precipitation in mm: between 0.0 and 99.9 percentile\n",
    "    'pr': (0.0, 44.53038024902344, 1.7398051, 4.482833),\n",
    "\n",
    "    # Specific humidity: between 0 and 1\n",
    "    'sph': (0., 1., 0.0071658953, 0.0042835088),\n",
    "\n",
    "    # Wind direction in degrees clockwise from north: between 0 and 360.\n",
    "    'th': (0., 360.0, 190.32976, 72.59854),\n",
    "\n",
    "    #Min temp: between 253.15 kelvin and 99.9 percentile\n",
    "    'tmmn': (253.15, 298.94891357421875, 281.08768, 8.982386),\n",
    "\n",
    "    #Max temp: between 253.15 kelvin and 99.9 percentile\n",
    "    'tmmx': (253.15, 315.09228515625, 295.17383, 9.815496),\n",
    "\n",
    "    # Wind speed in m/s: between 0. and 99.9 percentile\n",
    "    'vs': (0.0, 10.024310074806237, 3.8500874, 1.4109988),\n",
    "\n",
    "    # NFDRS fire danger index energy release component BTU's per square foot.\n",
    "    # 0., 99.9 percentile\n",
    "    'erc': (0.0, 106.24891662597656, 37.326267, 20.846027),\n",
    "\n",
    "    # Population density: between 0 and 99.9 percentile\n",
    "    'population': (0., 2534.06298828125, 25.531384, 154.72331),\n",
    "\n",
    "    # We don't want to normalize the FireMasks.\n",
    "    # 1 indicates fire, 0 no fire, -1 unlabeled data\n",
    "    'PrevFireMask': (-1., 1., 0., 1.),\n",
    "    'FireMask': (-1., 1., 0., 1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_features_dict(\n",
    "    sample_size: int,\n",
    "    features: List[Text],\n",
    ") -> Dict[Text, tf.io.FixedLenFeature]:\n",
    "  \"\"\"Creates a features dictionary for TensorFlow IO.\n",
    "\n",
    "  Args:\n",
    "    sample_size: Size of the input tiles (square).\n",
    "    features: List of feature names.\n",
    "\n",
    "  Returns:\n",
    "    A features dictionary for TensorFlow IO.\n",
    "  \"\"\"\n",
    "  sample_shape = [sample_size, sample_size]\n",
    "  features = set(features)\n",
    "  columns = [\n",
    "      tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n",
    "      for _ in features\n",
    "  ]\n",
    "  return dict(zip(features, columns))\n",
    "\n",
    "def _parse_fn(\n",
    "    example_proto: tf.train.Example, data_size: int, sample_size: int,\n",
    "    num_in_channels: int\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Reads a serialized example.\n",
    "\n",
    "  Args:\n",
    "    example_proto: A TensorFlow example protobuf.\n",
    "    data_size: Size of tiles (square) as read from input files.\n",
    "    sample_size: Size the tiles (square) when input into the model.\n",
    "    num_in_channels: Number of input channels.\n",
    "    clip_and_normalize: True if the data should be clipped and normalized.\n",
    "    clip_and_rescale: True if the data should be clipped and rescaled.\n",
    "    random_crop: True if the data should be randomly cropped.\n",
    "    center_crop: True if the data should be cropped in the center.\n",
    "\n",
    "  Returns:\n",
    "    (input_img, output_img) tuple of inputs and outputs to the ML model.\n",
    "  \"\"\"\n",
    "\n",
    "  input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n",
    "  feature_names = input_features + output_features\n",
    "  features_dict = _get_features_dict(data_size, feature_names)\n",
    "  features = tf.io.parse_single_example(example_proto, features_dict)\n",
    "\n",
    "\n",
    "  inputs_list = [features.get(key) for key in input_features]\n",
    "\n",
    "  inputs_stacked = tf.stack(inputs_list, axis=0)\n",
    "  input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n",
    "  outputs_list = [features.get(key) for key in output_features]\n",
    "  assert outputs_list, 'outputs_list should not be empty'\n",
    "  outputs_stacked = tf.stack(outputs_list, axis=0)\n",
    "\n",
    "  outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n",
    "  assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3'\n",
    "                                            'but dimensions of outputs_stacked'\n",
    "                                            f' are {outputs_stacked_shape}')\n",
    "  output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n",
    "\n",
    "  return input_img, output_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(filepattern):\n",
    "      dataset = tf.data.Dataset.list_files(filepattern)\n",
    "      dataset = dataset.interleave(\n",
    "      lambda x: tf.data.TFRecordDataset(x, compression_type=None),\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "      dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "      dataset = dataset.map(\n",
    "      lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n",
    "          x, 64, 32, 12),\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "      dataset = dataset.batch(100)\n",
    "      dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "      return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(file_pattern_train)\n",
    "test_dataset = load_dataset(file_pattern_test)\n",
    "eval_dataset = load_dataset(file_pattern_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16712/2587590083.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mX_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mX_eval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0my_eval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_temp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_eval = None\n",
    "y_eval = None\n",
    "while next(iter(eval_dataset)) != None:\n",
    "    if X_eval == None and y_eval == None:\n",
    "        X_eval, y_eval = next(iter(eval_dataset))\n",
    "    else:\n",
    "        X_temp, y_temp = next(iter(eval_dataset))\n",
    "        X_eval = tf.concat([X_eval,X_temp],axis=0)\n",
    "        y_eval = tf.concat([y_eval,y_temp], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 62, 62, 32)        3488      \n",
      "                                                                 \n",
      " average_pooling2d_6 (Averag  (None, 31, 31, 32)       0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 31, 31, 32)        0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " average_pooling2d_7 (Averag  (None, 14, 14, 64)       0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 84)                774228    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                5440      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 838,580\n",
      "Trainable params: 838,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3) ,activation='tanh', input_shape=(64, 64, 12)))\n",
    "model.add(layers.AveragePooling2D(2))\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='tanh'))\n",
    "model.add(layers.AveragePooling2D(2))\n",
    "model.add(layers.Activation('sigmoid'))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='tanh'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(84, activation='tanh'))\n",
    "model.add(layers.Dense(64, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1055, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1149, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\utils\\metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\metrics\\confusion_metrics.py\", line 1485, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\utils\\metrics_utils.py\", line 674, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 10) and (None, 64, 64, 1) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16712/2552408590.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     history = model.fit(X_train, y_train, epochs=10, \n\u001b[0m\u001b[0;32m     10\u001b[0m                     validation_data=(X_eval, y_eval))\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1055, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\", line 1149, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\utils\\metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\metrics\\confusion_metrics.py\", line 1485, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\annab\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\utils\\metrics_utils.py\", line 674, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 10) and (None, 64, 64, 1) are incompatible\n"
     ]
    }
   ],
   "source": [
    "#check diff optimizers and loss - \n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['AUC']) #change metrix to auc\n",
    "i = 0\n",
    "while next(iter(train_dataset)) != None:\n",
    "    i += 1\n",
    "    X_train, y_train = next(iter(train_dataset))\n",
    "    history = model.fit(X_train, y_train, epochs=10, \n",
    "                    validation_data=(X_eval, y_eval))\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
